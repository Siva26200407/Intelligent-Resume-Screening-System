# -*- coding: utf-8 -*-
"""Intelligent Resume Screening System(TN).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZpJLnLvdBvP8zns7txO5QdDcUbNP2R79
"""

!pip install nltk scikit-learn PyPDF2 python-docx spacy
!python -m spacy download en_core_web_sm

# ---------- 2. IMPORT LIBRARIES ----------
import nltk
import re
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

from PyPDF2 import PdfReader
from docx import Document
from google.colab import files  # For file upload in Colab

# ---------- 3. DOWNLOAD NLTK DATA ----------
nltk.download("punkt")       # ✅ Required for word_tokenize
nltk.download("stopwords")   # ✅ Required for stopwords
nltk.download("punkt_tab") # ✅ Required for sentence tokenization when using PunktTokenizer

# ---------- 4. GLOBAL VARIABLES ----------
STOP_WORDS = set(stopwords.words("english"))

SKILL_BANK = {
    "Data_Science": [
        "python", "sql", "machine learning", "statistics",
        "numpy", "pandas", "data analysis", "visualization"
    ],
    "Software_Development": [
        "java", "python", "c++", "git", "api",
        "spring", "oop", "software engineering"
    ],
    "Web_Development": [
        "html", "css", "javascript", "react",
        "node", "frontend", "backend"
    ],
    "Artificial_Intelligence": [
        "nlp", "deep learning", "tensorflow",
        "pytorch", "neural networks"
    ],
    "Business_Analytics": [
        "excel", "powerbi", "tableau",
        "business analysis", "reporting"
    ]
}

# =================================================
# 1. RESUME TEXT EXTRACTION (MULTI-FORMAT & PAGE CHECK)
# =================================================
def extract_resume_text(file_path):
    try:
        text = ""

        if file_path.endswith(".pdf"):
            reader = PdfReader(file_path)

            # Reject multi-page PDF
            if len(reader.pages) > 1:
                return None, "ERROR: Resume has more than 1 page. Only single-page resumes allowed."

            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + " "

        elif file_path.endswith(".docx"):
            doc = Document(file_path)

            # Rough single-page check (~50 paragraphs max)
            if len(doc.paragraphs) > 50:
                return None, "ERROR: Resume is too long. Only single-page resumes allowed."

            text = " ".join([p.text for p in doc.paragraphs])

        elif file_path.endswith(".txt"):
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read()

            # Rough single-page check (~50 lines max)
            if len(text.split("\n")) > 50:
                return None, "ERROR: Resume is too long. Only single-page resumes allowed."

        else:
            return None, "ERROR: Unsupported file format"

        if len(text.strip()) < 300:
            return None, "ERROR: Resume content too short or unreadable"

        return text, None

    except Exception as e:
        return None, f"ERROR: {str(e)}"

# =================================================
# 2. TEXT CLEANING
# =================================================
def clean_text(text):
    text = text.lower()
    text = re.sub(r"\d+", "", text)
    text = text.translate(str.maketrans("", "", string.punctuation))
    tokens = word_tokenize(text)
    tokens = [w for w in tokens if w not in STOP_WORDS and len(w) > 2]
    return " ".join(tokens)

# =================================================
# 3. DOMAIN INFERENCE
# =================================================
def infer_domain(resume_text):
    domain_scores = {}
    for domain, skills in SKILL_BANK.items():
        score = 0
        for skill in skills:
            if skill in resume_text:
                score += 1
        domain_scores[domain] = score
    detected_domain = max(domain_scores, key=domain_scores.get)
    return detected_domain, domain_scores

# =================================================
# 4. RESUME SCORING (ATS STYLE)
# =================================================
def calculate_resume_score(resume_text):
    words = resume_text.split()
    length_score = min(len(words) / 500, 1.0) * 30

    skill_score = 0
    for skills in SKILL_BANK.values():
        for skill in skills:
            skill_score += resume_text.count(skill) * 2

    unique_ratio = len(set(words)) / len(words)
    diversity_score = unique_ratio * 30

    final_score = length_score + skill_score + diversity_score
    return round(min(final_score, 100), 2)

# =================================================
# 5. FINAL RESUME ANALYSIS
# =================================================
def analyze_resume(file_path):
    raw_text, error = extract_resume_text(file_path)

    if error:
        return {
            "Status": "FAILED",
            "Reason": error
        }

    cleaned_text = clean_text(raw_text)
    domain, domain_distribution = infer_domain(cleaned_text)
    score = calculate_resume_score(cleaned_text)

    if score >= 70:
        shortlist = "STRONGLY SHORTLISTED"
    elif score >= 60:
        shortlist = "SHORTLISTED"
    else:
        shortlist = "NOT SHORTLISTED"

    return {
        "Status": "SUCCESS",
        "Detected_Domain": domain,
        "Domain_Score_Distribution": domain_distribution,
        "Resume_Score (%)": score,
        "Shortlist_Status": shortlist
    }

# =================================================
# 6. UPLOAD AND RUN ANALYSIS (GOOGLE COLAB)
# =================================================
if __name__ == "__main__":
    print("✅ Upload your single-page resume file (PDF, DOCX, or TXT)")
    uploaded_files = files.upload()  # Opens file chooser

    for filename in uploaded_files.keys():
        print(f"\nAnalyzing: {filename}")
        result = analyze_resume(filename)

        print("\n===== RESUME ANALYSIS RESULT =====\n")
        for key, value in result.items():
            print(f"{key}: {value}")